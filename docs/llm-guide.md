# LLM æ¨¡å—ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº† MyAgent æ¡†æ¶ä¸­ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ¨¡å—çš„ Python SDK ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

> ğŸ“– **ç›¸å…³æ–‡æ¡£**: å¦‚éœ€äº†è§£åç«¯ REST API æ¥å£ï¼Œè¯·å‚è€ƒ [åç«¯ API æ–‡æ¡£](backend-api.md)

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [é‡è¦å˜æ›´è¯´æ˜](#é‡è¦å˜æ›´è¯´æ˜)
- [å®‰è£…å’Œå¯¼å…¥](#å®‰è£…å’Œå¯¼å…¥)
- [åŸºæœ¬ä½¿ç”¨](#åŸºæœ¬ä½¿ç”¨)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ¯ æ¦‚è¿°

LLM æ¨¡å—æ˜¯ MyAgent çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„ç»Ÿä¸€ Python SDK æ¥å£ã€‚å®ƒæ”¯æŒï¼š

- å¤šç§ OpenAI å…¼å®¹çš„ API ç«¯ç‚¹ï¼ˆOpenAIã€DashScopeã€è‡ªå®šä¹‰æœåŠ¡ï¼‰
- æ™ºèƒ½çš„å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
- å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œé”™è¯¯å¤„ç†
- çµæ´»çš„é…ç½®é€‰é¡¹
- åŒæ­¥å’Œå¼‚æ­¥æ“ä½œæ”¯æŒ

## âš ï¸ é‡è¦å˜æ›´è¯´æ˜

**ä» v2.0 å¼€å§‹çš„é‡è¦å˜æ›´:**

1. **API å¯†é’¥å¿…éœ€**: æ‰€æœ‰ LLM æ“ä½œç°åœ¨éƒ½éœ€è¦æ˜¾å¼æä¾› API å¯†é’¥ï¼Œä¸å†ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–
2. **æ— å…¨å±€å®ä¾‹**: ç§»é™¤äº†å…¨å±€ LLM å®ä¾‹ï¼Œæ¯æ¬¡ä½¿ç”¨éƒ½éœ€è¦åˆ›å»ºå®ä¾‹å¹¶æä¾›å®Œæ•´é…ç½®
3. **é…ç½®çµæ´»æ€§**: æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€é…ç½®ï¼Œæ— éœ€é¢„è®¾å…¨å±€é…ç½®
4. **å¢å¼ºé”™è¯¯å¤„ç†**: æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¼‚å¸¸ç±»å‹

## ğŸ“¦ å®‰è£…å’Œå¯¼å…¥

### ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install openai python-dotenv
```

### å¯¼å…¥æ¨¡å—

```python
from src.core.engines.llm.base import LLM
```

## ğŸš€ åŸºæœ¬ä½¿ç”¨

### åˆ›å»º LLM å®ä¾‹

**æ³¨æ„**: ç°åœ¨å¿…é¡»æä¾› API å¯†é’¥

```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åŸºæœ¬é…ç½® (OpenAI)
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ä½¿ç”¨ GPT-4
llm = LLM(
    model="gpt-4",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ä½¿ç”¨ç¬¬ä¸‰æ–¹ API (å¦‚é˜¿é‡Œäº‘)
llm = LLM(
    model="qwen-turbo",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)
```

### åŸºæœ¬å¯¹è¯

#### å•è½®å¯¹è¯

```python
try:
    response = llm.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
    print(response)
except Exception as e:
    print(f"å¯¹è¯å¤±è´¥: {e}")
```

#### å¸¦ç³»ç»Ÿæç¤ºçš„å¯¹è¯

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

response = llm.chat(
    message="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIç ”ç©¶å‘˜ï¼Œè¯·ç”¨ç®€æ´æ˜“æ‡‚çš„è¯­è¨€å›ç­”é—®é¢˜"
)
print(response)
```

### ä½¿ç”¨ç¬¬ä¸‰æ–¹ API

#### DashScope (é˜¿é‡Œäº‘)

```python
llm = LLM(
    model="qwen-turbo",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

response = llm.chat("ä½ å¥½")
print(response)
```

#### è‡ªå®šä¹‰ OpenAI å…¼å®¹ç«¯ç‚¹

```python
llm = LLM(
    model="custom-model",
    api_key="your-custom-api-key",
    base_url="https://your-custom-endpoint.com/v1"
)

response = llm.chat("Hello")
print(response)
```



## ğŸ”§ é«˜çº§åŠŸèƒ½

### ä¸Šä¸‹æ–‡ç®¡ç†

#### ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ç¬¬ä¸€è½®å¯¹è¯
response1 = llm.chat("æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´25å²")
print(response1)

# ç¬¬äºŒè½®å¯¹è¯ï¼ˆæ¨¡å‹ä¼šè®°ä½ä¹‹å‰çš„ä¿¡æ¯ï¼‰
response2 = llm.chat("æˆ‘çš„å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ")
print(response2)  # æ¨¡å‹ä¼šå›ç­”ï¼šæ‚¨çš„å¹´é¾„æ˜¯25å²
```

#### ä¸ä¿æŒä¸Šä¸‹æ–‡

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# æ¯æ¬¡å¯¹è¯éƒ½æ˜¯ç‹¬ç«‹çš„
response1 = llm.chat("æˆ‘å«å¼ ä¸‰", keep_context=False)
response2 = llm.chat("æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ", keep_context=False)
print(response2)  # æ¨¡å‹ä¸ä¼šè®°ä½ä¹‹å‰çš„ä¿¡æ¯
```

#### æ‰‹åŠ¨ç®¡ç†ä¸Šä¸‹æ–‡

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# è·å–å½“å‰å¯¹è¯å†å²
history = llm.get_context()
print(f"å½“å‰æœ‰ {len(history)} æ¡æ¶ˆæ¯")

# æ¸…ç©ºå¯¹è¯å†å²
llm.clear_context()

# è®¾ç½®è‡ªå®šä¹‰å¯¹è¯å†å²
custom_history = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
]
llm.set_context(custom_history)
```

### æ¸©åº¦æ§åˆ¶

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# æ›´æœ‰åˆ›é€ æ€§çš„å›ç­”ï¼ˆtemperature è¶Šé«˜è¶Šéšæœºï¼‰
creative_response = llm.chat("å†™ä¸€é¦–è¯—", temperature=0.9)

# æ›´ç¡®å®šæ€§çš„å›ç­”ï¼ˆtemperature è¶Šä½è¶Šç¡®å®šï¼‰
factual_response = llm.chat("1+1ç­‰äºå¤šå°‘ï¼Ÿ", temperature=0.1)
```

## âš™ï¸ é…ç½®é€‰é¡¹

### Python SDK åˆå§‹åŒ–å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `model` | `str` | `"gpt-3.5-turbo"` | ä½¿ç”¨çš„æ¨¡å‹åç§° |
| `api_key` | `str` | **å¿…éœ€** | LLM æä¾›å•†çš„ API å¯†é’¥ |
| `base_url` | `str` | `None` | è‡ªå®šä¹‰ API ç«¯ç‚¹ |

**é‡è¦**: `api_key` ç°åœ¨æ˜¯å¿…éœ€å‚æ•°ï¼Œä¸å†ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–ã€‚

### chat æ–¹æ³•å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `user_input` | `str` | å¿…éœ€ | ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ |
| `system_prompt` | `str` | `None` | ç³»ç»Ÿæç¤ºè¯ |
| `keep_context` | `bool` | `True` | æ˜¯å¦ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ |
| `temperature` | `float` | `0.7` | æ§åˆ¶å›ç­”çš„éšæœºæ€§ |



### æ”¯æŒçš„æ¨¡å‹

#### OpenAI æ¨¡å‹
- `gpt-3.5-turbo`
- `gpt-4`
- `gpt-4-turbo`
- `gpt-4o`

#### ç¬¬ä¸‰æ–¹å…¼å®¹æ¨¡å‹
- é˜¿é‡Œäº‘ DashScope: `qwen-turbo`, `qwen-plus`, `qwen-max`
- å…¶ä»– OpenAI å…¼å®¹çš„ API ç«¯ç‚¹

### é…ç½®ç¤ºä¾‹

#### Python SDK é…ç½®
```python
# OpenAI
llm = LLM(
    model="gpt-4",
    api_key=os.getenv("OPENAI_API_KEY")
)

# é˜¿é‡Œäº‘ DashScope
llm = LLM(
    model="qwen-turbo",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# è‡ªå®šä¹‰ç«¯ç‚¹
llm = LLM(
    model="custom-model",
    api_key=os.getenv("CUSTOM_API_KEY"),
    base_url="https://your-endpoint.com/v1"
)
```



## ğŸ›¡ï¸ é”™è¯¯å¤„ç†

### Python SDK é”™è¯¯å¤„ç†

```python
try:
    llm = LLM(
        model="gpt-3.5-turbo",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    response = llm.chat("ä½ å¥½")
except ValueError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
except Exception as e:
    print(f"å‘ç”Ÿé”™è¯¯: {e}")
```

### å¸¸è§é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | æè¿° | å¤„ç†å»ºè®® |
|----------|------|----------|
| `ValueError` | é…ç½®å‚æ•°é”™è¯¯ | æ£€æŸ¥æ¨¡å‹åç§°å’ŒAPIå¯†é’¥æ ¼å¼ |
| `AuthenticationError` | è®¤è¯å¤±è´¥ | æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡® |
| `RateLimitError` | è¯·æ±‚é¢‘ç‡é™åˆ¶ | é™ä½è¯·æ±‚é¢‘ç‡æˆ–å‡çº§è´¦æˆ· |
| `APIError` | API æœåŠ¡é”™è¯¯ | é‡è¯•æˆ–è”ç³»æœåŠ¡æä¾›å•† |

### ç½‘ç»œè¶…æ—¶å¤„ç†

```python
import time

def chat_with_retry(llm, message, max_retries=3):
    for attempt in range(max_retries):
        try:
            return llm.chat(message)
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                raise e

# ä½¿ç”¨é‡è¯•æœºåˆ¶
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)
response = chat_with_retry(llm, "ä½ å¥½")
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½®ç³»ç»Ÿæç¤º

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# å¥½çš„ç³»ç»Ÿæç¤º
system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¼–ç¨‹åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. æä¾›æ¸…æ™°ã€å¯æ‰§è¡Œçš„ä»£ç ç¤ºä¾‹
2. è§£é‡Šä»£ç çš„å·¥ä½œåŸç†
3. æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œæœ€ä½³å®è·µ
4. ä½¿ç”¨ Python 3.10+ çš„ç‰¹æ€§
"""

response = llm.chat("å¦‚ä½•å®ç°å•ä¾‹æ¨¡å¼ï¼Ÿ", system_prompt=system_prompt)
```

### 2. ç®¡ç†å¯¹è¯é•¿åº¦

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# å®šæœŸæ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²
if len(llm.get_context()) > 20:  # è¶…è¿‡20æ¡æ¶ˆæ¯æ—¶æ¸…ç†
    # ä¿ç•™æœ€è¿‘çš„å‡ æ¡é‡è¦æ¶ˆæ¯
    recent_context = llm.get_context()[-10:]
    llm.clear_context()
    llm.set_context(recent_context)
```

### 3. ä½¿ç”¨é€‚å½“çš„æ¸©åº¦å€¼

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒçš„æ¸©åº¦
def get_temperature_by_task(task_type):
    temperatures = {
        "factual": 0.1,      # äº‹å®æ€§é—®é¢˜
        "creative": 0.8,     # åˆ›æ„æ€§ä»»åŠ¡
        "analysis": 0.3,     # åˆ†ææ€§ä»»åŠ¡
        "conversation": 0.7  # æ—¥å¸¸å¯¹è¯
    }
    return temperatures.get(task_type, 0.7)

# ä½¿ç”¨ç¤ºä¾‹
temp = get_temperature_by_task("factual")
response = llm.chat("Python çš„ GIL æ˜¯ä»€ä¹ˆï¼Ÿ", temperature=temp)
```

### 4. æ—¥å¿—ç›‘æ§

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# ç›‘æ§ API è°ƒç”¨
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)
response = llm.chat("ä½ å¥½")  # ä¼šè‡ªåŠ¨è®°å½•è¯¦ç»†çš„è°ƒç”¨æ—¥å¿—
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ModuleNotFoundError

```bash
# ç¡®ä¿æ­£ç¡®è®¾ç½® Python è·¯å¾„
export PYTHONPATH="${PYTHONPATH}:/path/to/myagent"

# æˆ–åœ¨ä»£ç ä¸­æ·»åŠ è·¯å¾„
import sys
sys.path.append('/path/to/myagent')
```

#### 2. API Key é”™è¯¯

```python
# æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®è®¾ç½®
import os
print("API Key:", os.getenv("OPENAI_API_KEY", "æœªè®¾ç½®"))

# æˆ–ç›´æ¥åœ¨ä»£ç ä¸­è®¾ç½®
llm = LLM(
    model="gpt-3.5-turbo",
    api_key="your-actual-api-key"
)
```

#### 3. ç½‘ç»œè¿æ¥é—®é¢˜

```python
# ä½¿ç”¨ä»£ç†
import os
os.environ["HTTP_PROXY"] = "http://your-proxy:port"
os.environ["HTTPS_PROXY"] = "https://your-proxy:port"

# æˆ–ä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://your-proxy-endpoint.com/v1"
)
```

#### 4. æ¨¡å‹ä¸å­˜åœ¨

```python
# æ£€æŸ¥å¯ç”¨æ¨¡å‹åˆ—è¡¨
available_models = [
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4-turbo-preview",
    # ç¬¬ä¸‰æ–¹æ¨¡å‹
    "qwen-turbo",
    "deepseek-chat"
]

# ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)
```

### è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.getLogger().setLevel(logging.DEBUG)

llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ç°åœ¨ä¼šçœ‹åˆ°è¯¦ç»†çš„ API è°ƒç”¨ä¿¡æ¯
response = llm.chat("ä½ å¥½")
```

#### æ£€æŸ¥è¯·æ±‚å’Œå“åº”

```python
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# æŸ¥çœ‹å¯¹è¯å†å²
history = llm.get_context()
for msg in history:
    print(f"{msg['role']}: {msg['content']}")
```

## ğŸ“š ç›¸å…³èµ„æº

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [é˜¿é‡Œäº‘ DashScope æ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [MyAgent API å‚è€ƒ](api-reference.md)
- [é¡¹ç›®æ¶æ„è¯´æ˜](architecture.md)

---

å¦‚æœé‡åˆ°å…¶ä»–é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®çš„ [Issues](https://github.com/your-repo/myagent/issues) æˆ–æäº¤æ–°çš„é—®é¢˜æŠ¥å‘Šã€‚