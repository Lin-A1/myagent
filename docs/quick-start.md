# å¿«é€Ÿå¼€å§‹æŒ‡å—

æ¬¢è¿ä½¿ç”¨ MyAgentï¼æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨ 5 åˆ†é’Ÿå†…å¿«é€Ÿä¸Šæ‰‹å¹¶è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ª LLM åº”ç”¨ã€‚

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
- [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
- [é…ç½®è®¾ç½®](#é…ç½®è®¾ç½®)
- [ç¬¬ä¸€ä¸ªç¤ºä¾‹](#ç¬¬ä¸€ä¸ªç¤ºä¾‹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ä¸‹ä¸€æ­¥](#ä¸‹ä¸€æ­¥)

## ğŸ”§ ç¯å¢ƒè¦æ±‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

- **Python**: 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- **æ“ä½œç³»ç»Ÿ**: Windows, macOS, æˆ– Linux
- **ç½‘ç»œ**: èƒ½å¤Ÿè®¿é—® OpenAI API æˆ–å…¶ä»–å…¼å®¹çš„ API ç«¯ç‚¹

### æ£€æŸ¥ Python ç‰ˆæœ¬

```bash
python --version
# æˆ–
python3 --version
```

å¦‚æœç‰ˆæœ¬ä½äº 3.10ï¼Œè¯·å…ˆå‡çº§ Pythonã€‚

## ğŸ“¦ å®‰è£…æ­¥éª¤

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-username/myagent.git
cd myagent
```

### 2. å®‰è£…ä¾èµ–

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ pip

```bash
pip install -r requirements.txt
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨é¡¹ç›®é…ç½®

```bash
pip install -e .
```

#### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ Poetryï¼ˆæ¨èï¼‰

```bash
# å®‰è£… Poetryï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
curl -sSL https://install.python-poetry.org | python3 -

# å®‰è£…é¡¹ç›®ä¾èµ–
poetry install
poetry shell  # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```

### 3. éªŒè¯å®‰è£…

```bash
python -c "from src.core.engines.llm.base import LLM; print('å®‰è£…æˆåŠŸï¼')"
```

## âš™ï¸ é…ç½®è®¾ç½®

### 1. è·å– API å¯†é’¥

#### OpenAI API

1. è®¿é—® [OpenAI Platform](https://platform.openai.com/)
2. æ³¨å†Œè´¦æˆ·å¹¶ç™»å½•
3. å‰å¾€ [API Keys](https://platform.openai.com/api-keys) é¡µé¢
4. ç‚¹å‡» "Create new secret key" åˆ›å»ºæ–°å¯†é’¥
5. å¤åˆ¶å¹¶ä¿å­˜å¯†é’¥ï¼ˆåªæ˜¾ç¤ºä¸€æ¬¡ï¼‰

#### å…¶ä»– API æä¾›å•†

- **é˜¿é‡Œäº‘ DashScope**: [è·å– API Key](https://help.aliyun.com/zh/dashscope/)
- **ç™¾åº¦åƒå¸†**: [è·å– API Key](https://cloud.baidu.com/product/wenxinworkshop)
- **è…¾è®¯æ··å…ƒ**: [è·å– API Key](https://cloud.tencent.com/product/hunyuan)

### 2. API å¯†é’¥ç®¡ç†

**é‡è¦å˜æ›´**: ä»æ–°ç‰ˆæœ¬å¼€å§‹ï¼ŒAPI å¯†é’¥ä¸å†ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–ï¼Œéœ€è¦åœ¨ä»£ç ä¸­æ˜¾å¼æä¾›ã€‚

#### æ¨èæ–¹å¼ï¼šä½¿ç”¨ .env æ–‡ä»¶

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# .env
OPENAI_API_KEY=your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1  # å¯é€‰
```

ç„¶ååœ¨ä»£ç ä¸­åŠ è½½ï¼š

```python
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å– API å¯†é’¥
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
```

#### å…¶ä»–æ–¹å¼

```python
# æ–¹å¼1ï¼šç›´æ¥åœ¨ä»£ç ä¸­è®¾ç½®ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
api_key = "your-api-key-here"

# æ–¹å¼2ï¼šä»é…ç½®æ–‡ä»¶è¯»å–
import json
with open("config.json") as f:
    config = json.load(f)
    api_key = config["api_key"]

# æ–¹å¼3ï¼šä»å‘½ä»¤è¡Œå‚æ•°è·å–
import sys
api_key = sys.argv[1] if len(sys.argv) > 1 else None
```

## ğŸš€ ç¬¬ä¸€ä¸ªç¤ºä¾‹

### 1. åŸºç¡€å¯¹è¯

åˆ›å»ºæ–‡ä»¶ `my_first_chat.py`ï¼š

```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

def main():
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è·å– API å¯†é’¥
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ :")
        print("OPENAI_API_KEY=your-api-key-here")
        return
    
    try:
        # åˆ›å»º LLM å®ä¾‹ï¼ˆç°åœ¨éœ€è¦æ˜¾å¼æä¾› API å¯†é’¥ï¼‰
        llm = LLM(
            model="gpt-3.5-turbo",
            api_key=api_key
        )
        
        print("ğŸ¤– MyAgent èŠå¤©æœºå™¨äººå¯åŠ¨ï¼")
        print("è¾“å…¥ 'quit' é€€å‡ºç¨‹åº\n")
        
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not user_input:
                continue
                
            try:
                # å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
                response = llm.chat(user_input)
                print(f"ğŸ¤– åŠ©æ‰‹: {response}\n")
                
            except Exception as e:
                print(f"âŒ èŠå¤©é”™è¯¯: {e}\n")
                
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()
```

### 2. è¿è¡Œç¤ºä¾‹

```bash
# ç¡®ä¿å·²åˆ›å»º .env æ–‡ä»¶å¹¶è®¾ç½® API å¯†é’¥
python my_first_chat.py
```

### 3. é¢„æœŸè¾“å‡º

```
ğŸ¤– MyAgent èŠå¤©æœºå™¨äººå¯åŠ¨ï¼
è¾“å…¥ 'quit' é€€å‡ºç¨‹åº

ğŸ‘¤ æ‚¨: ä½ å¥½
ğŸ¤– åŠ©æ‰‹: ä½ å¥½ï¼æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ

ğŸ‘¤ æ‚¨: ä½ èƒ½åšä»€ä¹ˆï¼Ÿ
ğŸ¤– åŠ©æ‰‹: æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š
1. å›ç­”å„ç§é—®é¢˜
2. ååŠ©å†™ä½œå’Œç¼–è¾‘
3. è§£é‡Šå¤æ‚æ¦‚å¿µ
4. æä¾›å»ºè®®å’Œæƒ³æ³•
5. è¿›è¡Œå¯¹è¯äº¤æµ

æœ‰ä»€ä¹ˆå…·ä½“éœ€è¦å¸®åŠ©çš„å—ï¼Ÿ

ğŸ‘¤ æ‚¨: quit
ğŸ‘‹ å†è§ï¼
```

### 4. REST API ç¤ºä¾‹

é™¤äº† Python SDKï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ REST APIï¼š

#### å¯åŠ¨æœåŠ¡å™¨

```bash
# å¯åŠ¨ API æœåŠ¡å™¨
python src/server/run_server.py
```

#### ä½¿ç”¨ curl æµ‹è¯•

```bash
# åŸºç¡€èŠå¤©
curl -X POST "http://localhost:8000/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ä½ å¥½",
    "config": {
      "model": "gpt-3.5-turbo",
      "api_key": "your-api-key-here"
    }
  }'
```

#### JavaScript å®¢æˆ·ç«¯ç¤ºä¾‹

```javascript
class LLMClient {
  constructor(baseUrl = 'http://localhost:8000') {
    this.baseUrl = baseUrl;
  }

  async chat(message, config, options = {}) {
    const response = await fetch(`${this.baseUrl}/llm/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        message,
        config,
        system_prompt: options.systemPrompt,
        keep_context: options.keepContext !== false,
        temperature: options.temperature || 0.7
      })
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    return await response.json();
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new LLMClient();
const config = {
  model: "gpt-3.5-turbo",
  api_key: "your-api-key-here"
};

client.chat("ä½ å¥½", config)
  .then(result => console.log(result.response))
  .catch(error => console.error('Error:', error));
```

## ğŸ”§ è¿›é˜¶ç¤ºä¾‹

### 1. ä½¿ç”¨ç³»ç»Ÿæç¤º

```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# åˆ›å»º LLM å®ä¾‹
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=api_key
)

# è®¾ç½®è§’è‰²å’Œè¡Œä¸º
response = llm.chat(
    user_input="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    system_prompt="ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„AIç ”ç©¶å‘˜ï¼Œè¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚"
)

print(response)
```

### 2. ä½¿ç”¨ç¬¬ä¸‰æ–¹ API

```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä½¿ç”¨é˜¿é‡Œäº‘ DashScope
dashscope_key = os.getenv("DASHSCOPE_API_KEY")  # ä»ç¯å¢ƒå˜é‡è·å–
llm = LLM(
    model="qwen-turbo",
    api_key=dashscope_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

response = llm.chat("ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”")
print(response)
```

### 3. ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡

```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

# åŠ è½½ç¯å¢ƒå˜é‡å¹¶åˆ›å»ºå®ä¾‹
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
llm = LLM(
    model="gpt-3.5-turbo",
    api_key=api_key
)

# å¼€å§‹å¯¹è¯
llm.chat("æˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜")
llm.chat("æˆ‘æ­£åœ¨å­¦ä¹  Python")

# æŸ¥çœ‹å¯¹è¯å†å²
history = llm.get_context()
print(f"å¯¹è¯å†å²: {len(history)} æ¡æ¶ˆæ¯")

# ç»§ç»­å¯¹è¯ï¼ˆæ¨¡å‹ä¼šè®°ä½ä¹‹å‰çš„ä¿¡æ¯ï¼‰
response = llm.chat("æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)  # åº”è¯¥å›ç­”ï¼šç¨‹åºå‘˜

# æ¸…ç©ºä¸Šä¸‹æ–‡
llm.clear_context()
```

### 4. REST API è¿›é˜¶ç”¨æ³•

#### ä¸Šä¸‹æ–‡ç®¡ç†

```bash
# è®¾ç½®ä¸Šä¸‹æ–‡
curl -X POST "http://localhost:8000/llm/context" \
  -H "Content-Type: application/json" \
  -d '{
    "context": [
      {"role": "user", "content": "æˆ‘å«å¼ ä¸‰"},
      {"role": "assistant", "content": "ä½ å¥½å¼ ä¸‰ï¼"}
    ]
  }'

# è·å–ä¸Šä¸‹æ–‡
curl -X GET "http://localhost:8000/llm/context"

# æ¸…ç©ºä¸Šä¸‹æ–‡
curl -X DELETE "http://localhost:8000/llm/context"
```

#### å¸¦é…ç½®çš„èŠå¤©

```bash
curl -X POST "http://localhost:8000/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "è§£é‡Šé‡å­è®¡ç®—",
    "config": {
      "model": "gpt-4",
      "api_key": "your-api-key-here"
    },
    "system_prompt": "ä½ æ˜¯ä¸€ä½ç‰©ç†å­¦æ•™æˆï¼Œè¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µ",
    "temperature": 0.3,
    "keep_context": true
  }'
```

## â“ å¸¸è§é—®é¢˜

### Q1: å‡ºç° "ModuleNotFoundError" é”™è¯¯

**é—®é¢˜**: `ModuleNotFoundError: No module named 'src'`

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ è·¯å¾„è®¾ç½®
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç„¶åå†å¯¼å…¥æ¨¡å—
from src.core.engines.llm.base import LLM
```

### Q2: API å¯†é’¥é”™è¯¯

**é—®é¢˜**: `ValueError: api_key is required` æˆ– `AuthenticationError: Invalid API key`

**è§£å†³æ–¹æ¡ˆ**:

#### æ£€æŸ¥ API å¯†é’¥è®¾ç½®
```python
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
print(f"API Key: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
print(f"API Key é•¿åº¦: {len(api_key) if api_key else 0}")
```

#### ç¡®ä¿ .env æ–‡ä»¶æ ¼å¼æ­£ç¡®
```bash
# .env æ–‡ä»¶å†…å®¹ï¼ˆæ³¨æ„æ²¡æœ‰ç©ºæ ¼ï¼‰
OPENAI_API_KEY=sk-your-actual-key-here
```

#### éªŒè¯ API å¯†é’¥æœ‰æ•ˆæ€§
```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

try:
    llm = LLM(model="gpt-3.5-turbo", api_key=api_key)
    response = llm.chat("æµ‹è¯•")
    print("âœ… API å¯†é’¥æœ‰æ•ˆ")
except Exception as e:
    print(f"âŒ API å¯†é’¥æ— æ•ˆ: {e}")
```

### Q3: ç½‘ç»œè¿æ¥é—®é¢˜

**é—®é¢˜**: `ConnectionError` æˆ–è¶…æ—¶é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
import os
from dotenv import load_dotenv
from src.core.engines.llm.base import LLM

# ä½¿ç”¨ä»£ç†
os.environ["HTTP_PROXY"] = "http://your-proxy:port"
os.environ["HTTPS_PROXY"] = "https://your-proxy:port"

# æˆ–ä½¿ç”¨å›½å†…é•œåƒ/è‡ªå®šä¹‰ç«¯ç‚¹
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

llm = LLM(
    model="gpt-3.5-turbo",
    api_key=api_key,
    base_url="https://your-mirror-endpoint.com/v1"
)
```

### Q4: æ¨¡å‹ä¸å­˜åœ¨

**é—®é¢˜**: `Model not found` é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
available_models = [
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4-turbo",
    "gpt-4o"
]

llm = LLM(
    model="gpt-3.5-turbo",  # ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹
    api_key=api_key
)
```

### Q5: REST API é”™è¯¯

**é—®é¢˜**: HTTP 4xx æˆ– 5xx é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:

#### æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
```bash
# æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æ£€æŸ¥ LLM æœåŠ¡ä¿¡æ¯
curl http://localhost:8000/llm/info
```

#### éªŒè¯è¯·æ±‚æ ¼å¼
```bash
# æ­£ç¡®çš„è¯·æ±‚æ ¼å¼
curl -X POST "http://localhost:8000/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ä½ å¥½",
    "config": {
      "model": "gpt-3.5-turbo",
      "api_key": "your-api-key-here"
    }
  }'
```

### Q6: æ—¥å¿—è¾“å‡ºå¤ªå¤š

**é—®é¢˜**: æ§åˆ¶å°è¾“å‡ºè¿‡å¤šæ—¥å¿—ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**:
```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger().setLevel(logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯

# æˆ–è€…ç¦ç”¨ç‰¹å®šæ¨¡å—çš„æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
```

### Q7: ç¯å¢ƒå˜é‡æœªç”Ÿæ•ˆ

**é—®é¢˜**: è®¾ç½®äº† .env æ–‡ä»¶ä½†ä»ç„¶æç¤º API å¯†é’¥æœªè®¾ç½®

**è§£å†³æ–¹æ¡ˆ**:

#### æ£€æŸ¥ .env æ–‡ä»¶ä½ç½®
```python
import os
from pathlib import Path

# .env æ–‡ä»¶åº”è¯¥åœ¨é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent  # æ ¹æ®è„šæœ¬ä½ç½®è°ƒæ•´
env_file = project_root / ".env"
print(f".env æ–‡ä»¶è·¯å¾„: {env_file}")
print(f".env æ–‡ä»¶å­˜åœ¨: {env_file.exists()}")
```

#### æ‰‹åŠ¨æŒ‡å®š .env æ–‡ä»¶è·¯å¾„
```python
from dotenv import load_dotenv
from pathlib import Path

# æŒ‡å®š .env æ–‡ä»¶è·¯å¾„
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
```

## ğŸ¯ ä¸‹ä¸€æ­¥

æ­å–œï¼æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº†ç¬¬ä¸€ä¸ª MyAgent åº”ç”¨ã€‚æ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š

### 1. æ·±å…¥å­¦ä¹ 

- ğŸ“– é˜…è¯» [LLM æ¨¡å—ä½¿ç”¨æŒ‡å—](llm-guide.md)
- ğŸ“š æŸ¥çœ‹ [API å‚è€ƒæ–‡æ¡£](api-reference.md)
- ğŸ—ï¸ äº†è§£ [é¡¹ç›®æ¶æ„](architecture.md)

### 2. æ¢ç´¢ç¤ºä¾‹

```bash
# è¿è¡Œå®˜æ–¹ç¤ºä¾‹
cd examples
python llm_chat.py
```

### 3. è‡ªå®šä¹‰å¼€å‘

- åˆ›å»ºè‡ªå·±çš„èŠå¤©æœºå™¨äºº
- é›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­
- å¼€å‘ç‰¹å®šé¢†åŸŸçš„åº”ç”¨

### 4. å‚ä¸è´¡çŒ®

- æŠ¥å‘Š Bug æˆ–æå‡ºå»ºè®®
- è´¡çŒ®ä»£ç æˆ–æ–‡æ¡£
- åˆ†äº«ä½¿ç”¨ç»éªŒ

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–å¸®åŠ©ï¼š

- ğŸ“– æŸ¥çœ‹ [æ–‡æ¡£](README.md)
- ğŸ› æäº¤ [Issue](https://github.com/your-repo/myagent/issues)
- ğŸ’¬ å‚ä¸ [è®¨è®º](https://github.com/your-repo/myagent/discussions)
- ğŸ“§ å‘é€é‚®ä»¶: support@myagent.com

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰