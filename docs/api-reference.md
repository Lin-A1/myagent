# API å‚è€ƒæ–‡æ¡£

æœ¬æ–‡æ¡£æä¾›äº† MyAgent æ¡†æ¶æ‰€æœ‰å…¬å¼€ API çš„è¯¦ç»†å‚è€ƒä¿¡æ¯ã€‚

## ğŸ“‹ ç›®å½•

- [REST API æ¥å£](#rest-api-æ¥å£)
  - [LLM æ¥å£](#llm-æ¥å£)
  - [OCR æ¥å£](#ocr-æ¥å£)
- [Python SDK](#python-sdk)
  - [LLM ç±»](#llm-ç±»)
  - [OCR ç±»](#ocr-ç±»)
- [æ•°æ®æ¨¡å‹](#æ•°æ®æ¨¡å‹)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [è®¤è¯å’Œé…ç½®](#è®¤è¯å’Œé…ç½®)

## ğŸŒ REST API æ¥å£

MyAgent æä¾›å®Œæ•´çš„ REST API æ¥å£ï¼Œæ”¯æŒé€šè¿‡ HTTP è¯·æ±‚ä¸æœåŠ¡è¿›è¡Œäº¤äº’ã€‚

### LLM æ¥å£

åŸºç¡€è·¯å¾„: `/llm`

#### POST `/llm/chat` - å¯¹è¯æ¥å£

ä¸ LLM è¿›è¡Œå¯¹è¯äº¤äº’ã€‚

**è¯·æ±‚ä½“:**
```json
{
  "message": "ä½ å¥½ï¼Œæˆ‘æ˜¯ç”¨æˆ·",
  "config": {
    "model": "gpt-3.5-turbo",
    "api_key": "your-api-key",
    "base_url": "https://api.openai.com/v1"
  },
  "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹",
  "keep_context": true,
  "temperature": 0.7
}
```

**å“åº”:**
```json
{
  "response": "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚",
  "context": [
    {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ç”¨æˆ·"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"}
  ],
  "model_info": {
    "model": "gpt-3.5-turbo",
    "timestamp": "2024-01-01T12:00:00Z"
  }
}
```

#### GET `/llm/context` - è·å–å¯¹è¯ä¸Šä¸‹æ–‡

è·å–å½“å‰å¯¹è¯çš„ä¸Šä¸‹æ–‡å†å²ã€‚

**æŸ¥è¯¢å‚æ•°:**
- `api_key` (å¿…éœ€): API å¯†é’¥
- `model` (å¯é€‰): æ¨¡å‹åç§°ï¼Œé»˜è®¤ "gpt-3.5-turbo"
- `base_url` (å¯é€‰): API åŸºç¡€ URL

**å“åº”:**
```json
{
  "context": [
    {"role": "user", "content": "ç”¨æˆ·æ¶ˆæ¯"},
    {"role": "assistant", "content": "åŠ©æ‰‹å›å¤"}
  ],
  "count": 2
}
```

#### POST `/llm/context` - è®¾ç½®å¯¹è¯ä¸Šä¸‹æ–‡

è®¾ç½®æˆ–æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

**è¯·æ±‚ä½“:**
```json
{
  "config": {
    "model": "gpt-3.5-turbo",
    "api_key": "your-api-key",
    "base_url": "https://api.openai.com/v1"
  },
  "context": [
    {"role": "user", "content": "ä¹‹å‰çš„å¯¹è¯"},
    {"role": "assistant", "content": "ä¹‹å‰çš„å›å¤"}
  ]
}
```

#### DELETE `/llm/context` - æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡

æ¸…ç©ºå½“å‰å¯¹è¯çš„æ‰€æœ‰ä¸Šä¸‹æ–‡ã€‚

**æŸ¥è¯¢å‚æ•°:**
- `api_key` (å¿…éœ€): API å¯†é’¥
- `model` (å¯é€‰): æ¨¡å‹åç§°
- `base_url` (å¯é€‰): API åŸºç¡€ URL

#### DELETE `/llm/context/last` - åˆ é™¤æœ€åä¸€è½®å¯¹è¯

åˆ é™¤æœ€åä¸€è½®é—®ç­”å¯¹è¯ã€‚

**æŸ¥è¯¢å‚æ•°:**
- `api_key` (å¿…éœ€): API å¯†é’¥
- `model` (å¯é€‰): æ¨¡å‹åç§°
- `base_url` (å¯é€‰): API åŸºç¡€ URL

#### GET `/llm/health` - å¥åº·æ£€æŸ¥

æ£€æŸ¥ LLM æœåŠ¡çš„å¥åº·çŠ¶æ€ã€‚

**å“åº”:**
```json
{
  "status": "healthy",
  "message": "LLM service is healthy (requires frontend config)",
  "timestamp": "2024-01-01T12:00:00Z"
}
```

#### GET `/llm/info` - æœåŠ¡ä¿¡æ¯

è·å– LLM æœåŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚

**å“åº”:**
```json
{
  "name": "MyAgent LLM Service",
  "version": "1.0.0",
  "description": "Large Language Model integration service",
  "supported_models": ["gpt-3.5-turbo", "gpt-4", "custom-models"],
  "configuration_required": {
    "api_key": "Required - Your LLM provider API key",
    "base_url": "Optional - Custom API endpoint URL",
    "temperature": "Optional - Response randomness (0.0-2.0)",
    "context": "Optional - Conversation context management"
  },
  "endpoints": ["/chat", "/context", "/health", "/info"]
}
```

### OCR æ¥å£

åŸºç¡€è·¯å¾„: `/ocr`

è¯¦ç»†çš„ OCR æ¥å£æ–‡æ¡£è¯·å‚è€ƒ [OCR ä½¿ç”¨æŒ‡å—](ocr-guide.md)ã€‚

## ğŸ Python SDK

### LLM ç±»

`src.core.engines.llm.base.LLM`

æ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯çš„ LLM å°è£…ç±»ï¼Œç°åœ¨éœ€è¦æ˜¾å¼æä¾›é…ç½®å‚æ•°ã€‚

#### æ„é€ å‡½æ•°

```python
def __init__(
    model: str = "gpt-3.5-turbo",
    api_key: str = None,
    base_url: str = None
) -> None
```

**å‚æ•°:**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `model` | `str` | `"gpt-3.5-turbo"` | ä½¿ç”¨çš„æ¨¡å‹åç§° |
| `api_key` | `str` | `None` | **å¿…éœ€** - LLM æä¾›å•†çš„ API å¯†é’¥ |
| `base_url` | `str` | `None` | è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼Œä¸ºç©ºæ—¶ä½¿ç”¨é»˜è®¤ç«¯ç‚¹ |

**é‡è¦å˜æ›´:**
- `api_key` ç°åœ¨æ˜¯å¿…éœ€å‚æ•°ï¼Œä¸å†ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–
- æ‰€æœ‰é…ç½®å¿…é¡»åœ¨å®ä¾‹åŒ–æ—¶æ˜ç¡®æä¾›

**ç¤ºä¾‹:**

```python
from src.core.engines.llm.base import LLM

# åŸºæœ¬ä½¿ç”¨ (ç°åœ¨éœ€è¦æä¾› API å¯†é’¥)
llm = LLM(
    model="gpt-3.5-turbo",
    api_key="your-openai-api-key"
)

# è‡ªå®šä¹‰æ¨¡å‹
llm = LLM(
    model="gpt-4",
    api_key="your-openai-api-key"
)

# ä½¿ç”¨ç¬¬ä¸‰æ–¹ API
llm = LLM(
    model="qwen-turbo",
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)
```

### æ–¹æ³•

#### `chat(user_input, system_prompt=None, keep_context=True, temperature=0.7)`

å‘é€ç”¨æˆ·è¾“å…¥å¹¶è¿”å›æ¨¡å‹å›å¤ã€‚

**å‚æ•°:**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `user_input` | `str` | å¿…éœ€ | ç”¨æˆ·æœ¬è½®è¾“å…¥å†…å®¹ |
| `system_prompt` | `Optional[str]` | `None` | ç³»ç»Ÿæç¤ºè¯ï¼Œä»…åœ¨é¦–æ¬¡å¯¹è¯æ—¶æœ‰æ•ˆ |
| `keep_context` | `bool` | `True` | æ˜¯å¦å°†æœ¬è½®å¯¹è¯è¿½åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ |
| `temperature` | `float` | `0.7` | ç”Ÿæˆæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›å¤çš„éšæœºæ€§ (0.0-2.0) |

**è¿”å›å€¼:**
- `str`: æ¨¡å‹çš„å›å¤å†…å®¹

**å¼‚å¸¸:**
- `Exception`: å½“ API è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

**è¡Œä¸ºè¯´æ˜:**
- å¦‚æœ `keep_context=True`ï¼Œç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤éƒ½ä¼šè¢«æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
- å¦‚æœ `keep_context=False`ï¼Œæœ¬è½®å¯¹è¯ä¸ä¼šå½±å“åç»­å¯¹è¯çš„ä¸Šä¸‹æ–‡
- å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›æ»šå·²æ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
- `system_prompt` åªåœ¨å¯¹è¯å†å²ä¸ºç©ºæ—¶ç”Ÿæ•ˆ

**ç¤ºä¾‹:**

```python
# åŸºæœ¬å¯¹è¯
response = llm.chat("ä½ å¥½")

# å¸¦ç³»ç»Ÿæç¤ºçš„å¯¹è¯
response = llm.chat(
    user_input="è§£é‡Šæœºå™¨å­¦ä¹ ",
    system_prompt="ä½ æ˜¯ä¸€ä½AIä¸“å®¶ï¼Œè¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"
)

# ä¸ä¿æŒä¸Šä¸‹æ–‡çš„å¯¹è¯
response = llm.chat("è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜", keep_context=False)

# è°ƒæ•´åˆ›é€ æ€§
creative_response = llm.chat("å†™ä¸€é¦–è¯—", temperature=0.9)
factual_response = llm.chat("1+1ç­‰äºå¤šå°‘", temperature=0.1)
```

#### `clear_context()`

æ¸…ç©ºå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

**å‚æ•°:**
- æ— 

**è¿”å›å€¼:**
- `None`

**ç¤ºä¾‹:**

```python
# æ¸…ç©ºå¯¹è¯å†å²
llm.clear_context()
```

#### `get_context()`

è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡çš„å‰¯æœ¬ã€‚

**å‚æ•°:**
- æ— 

**è¿”å›å€¼:**
- `List[Dict[str, str]]`: å¯¹è¯å†å²çš„å‰¯æœ¬ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« `role` å’Œ `content` é”®

**ç¤ºä¾‹:**

```python
# è·å–å¯¹è¯å†å²
history = llm.get_context()
for message in history:
    print(f"{message['role']}: {message['content']}")
```

#### `set_context(history)`

æ‰‹åŠ¨è®¾ç½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

**å‚æ•°:**

| å‚æ•° | ç±»å‹ | æè¿° |
|------|------|------|
| `history` | `List[Dict[str, str]]` | è¦è®¾ç½®çš„å¯¹è¯å†å²è®°å½• |

**è¿”å›å€¼:**
- `None`

**æ¶ˆæ¯æ ¼å¼:**
æ¯ä¸ªæ¶ˆæ¯å­—å…¸å¿…é¡»åŒ…å«ä»¥ä¸‹é”®ï¼š
- `role`: æ¶ˆæ¯è§’è‰²ï¼Œå¯é€‰å€¼ä¸º `"system"`, `"user"`, `"assistant"`
- `content`: æ¶ˆæ¯å†…å®¹

**ç¤ºä¾‹:**

```python
# è®¾ç½®è‡ªå®šä¹‰å¯¹è¯å†å²
custom_history = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
]
llm.set_context(custom_history)
```

### å±æ€§

#### `model`

**ç±»å‹:** `str`  
**æè¿°:** å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°

#### `messages`

**ç±»å‹:** `List[Dict[str, str]]`  
**æè¿°:** å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨  
**æ³¨æ„:** å»ºè®®ä½¿ç”¨ `get_context()` æ–¹æ³•è·å–å‰¯æœ¬ï¼Œè€Œä¸æ˜¯ç›´æ¥è®¿é—®æ­¤å±æ€§

#### `logger`

**ç±»å‹:** `logging.Logger`  
**æè¿°:** æ—¥å¿—è®°å½•å™¨å®ä¾‹  
**æ³¨æ„:** ç”¨äºå†…éƒ¨æ—¥å¿—è®°å½•ï¼Œé€šå¸¸ä¸éœ€è¦ç›´æ¥ä½¿ç”¨

#### `client`

**ç±»å‹:** `openai.OpenAI`  
**æè¿°:** OpenAI å®¢æˆ·ç«¯å®ä¾‹  
**æ³¨æ„:** å†…éƒ¨ä½¿ç”¨ï¼Œä¸å»ºè®®ç›´æ¥è®¿é—®

## ğŸ” OCR ç±»

`src.core.engines.ocr.base.OCR`

å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å®¢æˆ·ç«¯ç±»ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œæä¾›ç®€æ´çš„æ–‡å­—è¯†åˆ«æ¥å£ã€‚

### æ„é€ å‡½æ•°

#### `__init__(server_url="http://localhost:8001")`

åˆå§‹åŒ– OCR å®¢æˆ·ç«¯å®ä¾‹ã€‚

**å‚æ•°:**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `server_url` | `str` | `"http://localhost:8001"` | OCR æœåŠ¡å™¨åœ°å€ |

**è¿”å›å€¼:**
- `None`

**ç¤ºä¾‹:**

```python
from src.core.engines.ocr.base import OCR

# ä½¿ç”¨é»˜è®¤æœåŠ¡å™¨åœ°å€
ocr = OCR()

# ä½¿ç”¨è‡ªå®šä¹‰æœåŠ¡å™¨åœ°å€
ocr = OCR(server_url="http://192.168.1.100:8001")
```

### æ–¹æ³•

#### `recognize(image_input)`

è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ã€‚

**å‚æ•°:**

| å‚æ•° | ç±»å‹ | æè¿° |
|------|------|------|
| `image_input` | `Union[str, np.ndarray, bytes]` | å›¾ç‰‡è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼ |

**è¿”å›å€¼:**
- `List[Any]`: OCR è¯†åˆ«ç»“æœåˆ—è¡¨

**å¼‚å¸¸:**
- `ValueError`: å½“è¾“å…¥æ ¼å¼ä¸æ”¯æŒæˆ–æ–‡ä»¶ä¸å­˜åœ¨æ—¶
- `requests.exceptions.RequestException`: å½“ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶
- `Exception`: å…¶ä»– OCR å¤„ç†å¼‚å¸¸

**ç¤ºä¾‹:**

```python
# ä½¿ç”¨æ–‡ä»¶è·¯å¾„
result = ocr.recognize('/path/to/image.jpg')

# ä½¿ç”¨ base64 å­—ç¬¦ä¸²
result = ocr.recognize('iVBORw0KGgoAAAANSUhEUgAA...')

# ä½¿ç”¨ numpy æ•°ç»„
import cv2
image = cv2.imread('image.jpg')
result = ocr.recognize(image)

# ä½¿ç”¨å­—èŠ‚æ•°æ®
with open('image.jpg', 'rb') as f:
    image_bytes = f.read()
result = ocr.recognize(image_bytes)
```

### æ”¯æŒçš„è¾“å…¥æ ¼å¼

#### 1. æ–‡ä»¶è·¯å¾„ (str)
```python
ocr.recognize('/home/user/document.png')
ocr.recognize('./images/receipt.jpg')
```

#### 2. Base64 å­—ç¬¦ä¸² (str)
```python
# çº¯ base64 å­—ç¬¦ä¸²
ocr.recognize('iVBORw0KGgoAAAANSUhEUgAA...')

# Data URL æ ¼å¼
ocr.recognize('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...')
```

#### 3. NumPy æ•°ç»„ (np.ndarray)
```python
import cv2
import numpy as np

# OpenCV è¯»å–çš„å›¾ç‰‡
image = cv2.imread('document.jpg')
result = ocr.recognize(image)

# PIL è½¬æ¢çš„æ•°ç»„
from PIL import Image
pil_image = Image.open('document.jpg')
np_array = np.array(pil_image)
result = ocr.recognize(np_array)
```

#### 4. å­—èŠ‚æ•°æ® (bytes)
```python
# ç›´æ¥è¯»å–æ–‡ä»¶å­—èŠ‚
with open('document.pdf', 'rb') as f:
    image_bytes = f.read()
result = ocr.recognize(image_bytes)

# ç½‘ç»œä¸‹è½½çš„å›¾ç‰‡
import requests
response = requests.get('https://example.com/image.jpg')
result = ocr.recognize(response.content)
```

### æ—¥å¿—è®°å½•

OCR ç±»é›†æˆäº†è¯¦ç»†çš„æ—¥å¿—è®°å½•åŠŸèƒ½ï¼š

```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
2024-01-01 10:00:00 - OCR - INFO - å¼€å§‹ OCR è¯†åˆ«ï¼Œè¾“å…¥ç±»å‹: str
2024-01-01 10:00:00 - OCR - DEBUG - è¯»å–å›¾ç‰‡æ–‡ä»¶: /path/to/image.jpg
2024-01-01 10:00:00 - OCR - DEBUG - æˆåŠŸè¯»å–å›¾ç‰‡æ–‡ä»¶ï¼Œå¤§å°: 102400 å­—èŠ‚
2024-01-01 10:00:00 - OCR - DEBUG - å›¾ç‰‡è½¬æ¢ä¸º base64 å®Œæˆ
2024-01-01 10:00:00 - OCR - DEBUG - å‘é€ OCR è¯·æ±‚åˆ°: http://localhost:8001/ocr
2024-01-01 10:00:01 - OCR - INFO - OCR è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«åˆ° 5 ä¸ªæ–‡æœ¬åŒºåŸŸ
```

### é”™è¯¯å¤„ç†

```python
try:
    result = ocr.recognize('nonexistent.jpg')
except ValueError as e:
    print(f"è¾“å…¥é”™è¯¯: {e}")
except requests.exceptions.ConnectionError:
    print("æ— æ³•è¿æ¥åˆ° OCR æœåŠ¡å™¨")
except requests.exceptions.Timeout:
    print("OCR è¯·æ±‚è¶…æ—¶")
except Exception as e:
    print(f"OCR è¯†åˆ«å¤±è´¥: {e}")
```

## ğŸ“ æ—¥å¿—ç³»ç»Ÿ

### æ—¥å¿—çº§åˆ«

MyAgent ä½¿ç”¨æ ‡å‡†çš„ Python æ—¥å¿—çº§åˆ«ï¼š

- `DEBUG`: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
- `INFO`: ä¸€èˆ¬ä¿¡æ¯
- `WARNING`: è­¦å‘Šä¿¡æ¯
- `ERROR`: é”™è¯¯ä¿¡æ¯
- `CRITICAL`: ä¸¥é‡é”™è¯¯

### æ—¥å¿—é…ç½®

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger().setLevel(logging.INFO)

# æŸ¥çœ‹è¯¦ç»†è°ƒè¯•ä¿¡æ¯
logging.getLogger().setLevel(logging.DEBUG)
```

### æ—¥å¿—è¾“å‡ºç¤ºä¾‹

```
2025-10-26 02:52:34 - LLM - INFO - LLM initialized with model: gpt-3.5-turbo
2025-10-26 02:52:34 - LLM - INFO - Starting chat with user input length: 5
2025-10-26 02:52:35 - LLM - INFO - Received response from model. Reply length: 58
```

## âš ï¸ å¼‚å¸¸å¤„ç†

### å¸¸è§å¼‚å¸¸ç±»å‹

#### OpenAI API å¼‚å¸¸

```python
from openai import OpenAIError, AuthenticationError, RateLimitError

try:
    response = llm.chat("ä½ å¥½")
except AuthenticationError:
    print("API å¯†é’¥æ— æ•ˆ")
except RateLimitError:
    print("API è°ƒç”¨é¢‘ç‡è¶…é™")
except OpenAIError as e:
    print(f"OpenAI API é”™è¯¯: {e}")
```

#### ç½‘ç»œå¼‚å¸¸

```python
import requests

try:
    response = llm.chat("ä½ å¥½")
except requests.exceptions.ConnectionError:
    print("ç½‘ç»œè¿æ¥å¤±è´¥")
except requests.exceptions.Timeout:
    print("è¯·æ±‚è¶…æ—¶")
```

#### é€šç”¨å¼‚å¸¸å¤„ç†

```python
try:
    response = llm.chat("ä½ å¥½")
except Exception as e:
    print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    # æ£€æŸ¥æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯
```

## ğŸ“Š ç±»å‹å®šä¹‰

### æ¶ˆæ¯ç±»å‹

```python
from typing import Dict, List, Literal

MessageRole = Literal["system", "user", "assistant"]

Message = Dict[str, str]  # {"role": MessageRole, "content": str}

ConversationHistory = List[Message]
```

### æ¨¡å‹å‚æ•°ç±»å‹

```python
from typing import Optional

class ChatParameters:
    user_input: str
    system_prompt: Optional[str] = None
    keep_context: bool = True
    temperature: float = 0.7  # èŒƒå›´: 0.0 - 2.0
```

## ğŸ”§ é…ç½®å¸¸é‡

### é»˜è®¤å€¼

```python
DEFAULT_MODEL = "gpt-3.5-turbo"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 2048
DEFAULT_TOP_P = 1
DEFAULT_FREQUENCY_PENALTY = 0
DEFAULT_PRESENCE_PENALTY = 0
```

### ç¯å¢ƒå˜é‡

| å˜é‡å | æè¿° | ç¤ºä¾‹ |
|--------|------|------|
| `OPENAI_API_KEY` | OpenAI API å¯†é’¥ | `sk-...` |
| `OPENAI_BASE_URL` | è‡ªå®šä¹‰ API ç«¯ç‚¹ | `https://api.openai.com/v1` |

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### LLM å®Œæ•´ç¤ºä¾‹

```python
import os
from src.core.engines.llm.base import LLM

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "your-api-key"

# åˆ›å»º LLM å®ä¾‹
llm = LLM(model="gpt-3.5-turbo")

try:
    # å¼€å§‹å¯¹è¯
    response1 = llm.chat(
        user_input="ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"
    )
    print("åŠ©æ‰‹:", response1)
    
    # ç»§ç»­å¯¹è¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
    response2 = llm.chat("æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("åŠ©æ‰‹:", response2)
    
    # æŸ¥çœ‹å¯¹è¯å†å²
    history = llm.get_context()
    print(f"å¯¹è¯å†å²åŒ…å« {len(history)} æ¡æ¶ˆæ¯")
    
    # æ¸…ç©ºä¸Šä¸‹æ–‡
    llm.clear_context()
    
except Exception as e:
    print(f"é”™è¯¯: {e}")
```

### OCR å®Œæ•´ç¤ºä¾‹

```python
from src.core.engines.ocr.base import OCR
import cv2
import numpy as np

# åˆ›å»º OCR å®ä¾‹
ocr = OCR(server_url="http://localhost:8001")

try:
    # æ–¹å¼1: ä½¿ç”¨æ–‡ä»¶è·¯å¾„
    result1 = ocr.recognize('/path/to/document.jpg')
    print("æ–‡ä»¶è·¯å¾„è¯†åˆ«ç»“æœ:", result1)
    
    # æ–¹å¼2: ä½¿ç”¨ OpenCV è¯»å–çš„å›¾ç‰‡
    image = cv2.imread('/path/to/document.jpg')
    result2 = ocr.recognize(image)
    print("NumPy æ•°ç»„è¯†åˆ«ç»“æœ:", result2)
    
    # æ–¹å¼3: ä½¿ç”¨ base64 å­—ç¬¦ä¸²
    import base64
    with open('/path/to/document.jpg', 'rb') as f:
        image_data = f.read()
        b64_string = base64.b64encode(image_data).decode('utf-8')
    
    result3 = ocr.recognize(b64_string)
    print("Base64 è¯†åˆ«ç»“æœ:", result3)
    
    # æ–¹å¼4: ä½¿ç”¨å­—èŠ‚æ•°æ®
    with open('/path/to/document.jpg', 'rb') as f:
        image_bytes = f.read()
    
    result4 = ocr.recognize(image_bytes)
    print("å­—èŠ‚æ•°æ®è¯†åˆ«ç»“æœ:", result4)
    
except ValueError as e:
    print(f"è¾“å…¥æ ¼å¼é”™è¯¯: {e}")
except Exception as e:
    print(f"OCR è¯†åˆ«å¤±è´¥: {e}")
```

### ç»„åˆä½¿ç”¨ç¤ºä¾‹

```python
from src.core.engines.llm.base import LLM
from src.core.engines.ocr.base import OCR

# åˆå§‹åŒ– (æ³¨æ„ï¼šLLM ç°åœ¨éœ€è¦ API å¯†é’¥)
llm = LLM(
    model="gpt-3.5-turbo",
    api_key="your-api-key"
)
ocr = OCR()

# OCR è¯†åˆ«æ–‡æ¡£
try:
    ocr_result = ocr.recognize('/path/to/receipt.jpg')
    
    # æå–è¯†åˆ«åˆ°çš„æ–‡æœ¬
    extracted_text = ""
    for item in ocr_result:
        if isinstance(item, list) and len(item) >= 2:
            # PaddleOCR æ ¼å¼: [bbox, (text, confidence)]
            text = item[1][0] if isinstance(item[1], tuple) else str(item[1])
            extracted_text += text + "\n"
    
    # ä½¿ç”¨ LLM åˆ†æ OCR ç»“æœ
    analysis = llm.chat(
        user_input=f"è¯·åˆ†æè¿™å¼ æ”¶æ®çš„å†…å®¹ï¼š\n{extracted_text}",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªè´¢åŠ¡åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†ææ”¶æ®å’Œå‘ç¥¨ã€‚"
    )
    
    print("OCR è¯†åˆ«ç»“æœ:")
    print(extracted_text)
    print("\nLLM åˆ†æç»“æœ:")
    print(analysis)
    
except Exception as e:
    print(f"å¤„ç†å¤±è´¥: {e}")
```

## ğŸ“Š æ•°æ®æ¨¡å‹

### LLM ç›¸å…³æ¨¡å‹

#### LLMConfig
```python
class LLMConfig(BaseModel):
    model: str                    # æ¨¡å‹åç§°
    api_key: str                 # API å¯†é’¥
    base_url: Optional[str] = None  # è‡ªå®šä¹‰ API ç«¯ç‚¹
```

#### ChatRequest
```python
class ChatRequest(BaseModel):
    message: str                 # ç”¨æˆ·æ¶ˆæ¯
    config: LLMConfig           # LLM é…ç½®
    system_prompt: Optional[str] = None  # ç³»ç»Ÿæç¤ºè¯
    keep_context: bool = True    # æ˜¯å¦ä¿æŒä¸Šä¸‹æ–‡
    temperature: float = 0.7     # æ¸©åº¦å‚æ•°
```

#### ChatResponse
```python
class ChatResponse(BaseModel):
    response: str               # LLM å›å¤
    context: List[Dict[str, str]]  # å¯¹è¯ä¸Šä¸‹æ–‡
    model_info: Dict[str, Any]  # æ¨¡å‹ä¿¡æ¯
```

#### ContextRequest
```python
class ContextRequest(BaseModel):
    config: LLMConfig           # LLM é…ç½®
    context: Optional[List[Dict[str, str]]] = None  # ä¸Šä¸‹æ–‡æ•°æ®
```

#### ContextResponse
```python
class ContextResponse(BaseModel):
    context: List[Dict[str, str]]  # ä¸Šä¸‹æ–‡æ•°æ®
    count: int                  # æ¶ˆæ¯æ•°é‡
```

## âš ï¸ é”™è¯¯å¤„ç†

### HTTP çŠ¶æ€ç 

| çŠ¶æ€ç  | æè¿° | å¸¸è§åŸå›  |
|--------|------|----------|
| 200 | æˆåŠŸ | è¯·æ±‚æ­£å¸¸å¤„ç† |
| 400 | è¯·æ±‚é”™è¯¯ | å‚æ•°æ ¼å¼é”™è¯¯ã€ç¼ºå°‘å¿…éœ€å‚æ•° |
| 401 | è®¤è¯å¤±è´¥ | API å¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ |
| 422 | éªŒè¯é”™è¯¯ | è¯·æ±‚ä½“æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ |
| 500 | æœåŠ¡å™¨é”™è¯¯ | LLM æœåŠ¡å¼‚å¸¸ã€ç½‘ç»œè¿æ¥å¤±è´¥ |

### å¸¸è§é”™è¯¯ç±»å‹

#### 1. é…ç½®é”™è¯¯
```json
{
  "detail": "Invalid API key provided"
}
```

#### 2. ç½‘ç»œé”™è¯¯
```json
{
  "detail": "Failed to connect to LLM service"
}
```

#### 3. å‚æ•°éªŒè¯é”™è¯¯
```json
{
  "detail": [
    {
      "loc": ["body", "config", "api_key"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ]
}
```

## ğŸ” è®¤è¯å’Œé…ç½®

### API å¯†é’¥ç®¡ç†

**é‡è¦å˜æ›´**: ä» v2.0 å¼€å§‹ï¼Œæ‰€æœ‰ LLM ç›¸å…³æ“ä½œéƒ½éœ€è¦åœ¨è¯·æ±‚ä¸­æä¾› API å¯†é’¥ï¼Œä¸å†æ”¯æŒä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–ã€‚

#### REST API è®¤è¯
- **æŸ¥è¯¢å‚æ•°**: åœ¨ GET/DELETE è¯·æ±‚ä¸­é€šè¿‡ `api_key` å‚æ•°æä¾›
- **è¯·æ±‚ä½“**: åœ¨ POST è¯·æ±‚ä¸­é€šè¿‡ `config.api_key` å­—æ®µæä¾›

#### Python SDK è®¤è¯
```python
# å¿…é¡»æ˜¾å¼æä¾› API å¯†é’¥
llm = LLM(
    model="gpt-3.5-turbo",
    api_key="your-api-key"  # å¿…éœ€å‚æ•°
)
```

### æ”¯æŒçš„ LLM æä¾›å•†

| æä¾›å•† | é»˜è®¤ base_url | ç¤ºä¾‹æ¨¡å‹ |
|--------|---------------|----------|
| OpenAI | `https://api.openai.com/v1` | `gpt-3.5-turbo`, `gpt-4` |
| é˜¿é‡Œäº‘ | `https://dashscope.aliyuncs.com/compatible-mode/v1` | `qwen-turbo`, `qwen-plus` |
| è‡ªå®šä¹‰ | ç”¨æˆ·æŒ‡å®š | æ ¹æ®æœåŠ¡å•†è€Œå®š |

### é…ç½®æœ€ä½³å®è·µ

1. **å®‰å…¨æ€§**: ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API å¯†é’¥
2. **ç¯å¢ƒéš”ç¦»**: ä¸ºä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒçš„ API å¯†é’¥
3. **é”™è¯¯å¤„ç†**: å§‹ç»ˆå¤„ç†è®¤è¯å¤±è´¥çš„æƒ…å†µ
4. **è¿æ¥è¶…æ—¶**: è®¾ç½®åˆç†çš„ç½‘ç»œè¶…æ—¶æ—¶é—´

---

æ›´å¤šä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒ [LLM æ¨¡å—ä½¿ç”¨æŒ‡å—](llm-guide.md) å’Œ [å¿«é€Ÿå¼€å§‹æŒ‡å—](quick-start.md)ã€‚